# Inferenceï¼ˆåŽŸ Kuzcoï¼‰Epoch 2 æœ€æ–°ä¸€é”®è‡ªåŠ¨é‡å¯ + å¤šå¼€è„šæœ¬

**ä½œè€…ï¼šJ1Nï¼ŒKuzCommunityCN Founder**  
**æŽ¨ç‰¹ï¼š[ @J1N226 ](https://twitter.com/J1N226)**ï¼ˆå…³æ³¨æŽ¨ç‰¹ç§ä¿¡æ‹‰äº¤æµç¾¤ï¼‰

---

## ðŸ“Œ é¡¹ç›®ç®€ä»‹

æœ¬è„šæœ¬ä¸º Inference Epoch 2 æä¾›äº† **ä¸€é”®éƒ¨ç½²ã€è‡ªåŠ¨é‡å¯å’Œå¤šå®žä¾‹è¿è¡Œ** çš„å®Œæ•´æ–¹æ¡ˆã€‚  
åŸºäºŽ [Singosol åŽŸé¡¹ç›®](https://github.com/singosol/kuzco-docker) è¿›è¡Œ Forkï¼Œå¹¶æ ¹æ®å®žæµ‹å¯¹å®˜æ–¹ Epoch 2 çš„æ›´æ–°è¿›è¡Œäº†é€‚é…ä¸Žä¼˜åŒ–ã€‚

ä¸»è¦ä¼˜åŒ–å†…å®¹åŒ…æ‹¬ï¼š
- è‡ªåŠ¨è¯†åˆ« GPU æ˜¾å­˜
- æ ¹æ®æ˜¾å­˜è‡ªåŠ¨å¤šå¼€è¿è¡Œ
- è„šæœ¬å†…å«è¯¦ç»†æ³¨é‡Šï¼Œæ–¹ä¾¿æ ¹æ®ä¸åŒè®¾å¤‡çµæ´»è°ƒæ•´å‚æ•°

---

## âœ… çŽ¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šLinux  
- **æ˜¾å¡é©±åŠ¨**ï¼šNVIDIA 550  
- **æ˜¾å­˜è¦æ±‚**ï¼šæ˜¾å­˜å¤§äºŽ 6GBï¼ˆè¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.inference.supply/hardware)ï¼‰  
- **Python**ï¼š3.8 åŠä»¥ä¸Šç‰ˆæœ¬
- **Docker**ï¼šæœ€æ–°ç‰ˆ

---

## ðŸš€ è„šæœ¬åŠŸèƒ½

- ðŸ” **è‡ªåŠ¨é‡å¯**ï¼šæ— éœ€äººå·¥å¹²é¢„ï¼ŒæŒç»­ç¨³å®šè¿è¡Œ  
- ðŸ§© **å¤šå¼€æ”¯æŒ**ï¼šæ ¹æ®æ˜¾å­˜è‡ªåŠ¨åˆ†é…çº¿ç¨‹æ•°é‡  
  - ä¾‹å¦‚ï¼š6GB æ˜¾å­˜ = 1 çº¿ç¨‹ï¼Œ12GB æ˜¾å­˜ = 2 çº¿ç¨‹ï¼Œä»¥æ­¤ç±»æŽ¨

---

## ðŸ› ï¸ ä½¿ç”¨æ–¹æ³•

åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹æŒ‡ä»¤è¿è¡Œè„šæœ¬ï¼š

```bash
python3 kzco.py -c "--worker XXX --code XXX"
```

> å‚æ•° `"--worker XXX --code XXX"` æ¥è‡ªå®˜æ–¹ Inference å¹³å°åˆ›å»º Worker æ—¶é€‰æ‹© Docker æ–¹å¼æ‰€è‡ªåŠ¨ç”Ÿæˆçš„å‘½ä»¤ã€‚

å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯ä½ ä¿®æ”¹åŽçš„è¯´æ˜Žçš„è‹±æ–‡ç¿»è¯‘ç‰ˆï¼Œæ ¼å¼å’Œé£Žæ ¼ä¿æŒä¸“ä¸šæ¸…æ™°ï¼Œé€‚åˆç›´æŽ¥ç”¨äºŽ GitHub READMEï¼š

---

# Inference (formerly Kuzco) Epoch 2 One-Click Auto-Restart & Multi-Instance Script

**Author: J1N, Founder of KuzCommunityCN**  
**Twitter: [@J1N226](https://twitter.com/J1N226)** 

---

## ðŸ“Œ Overview

This script provides a **complete solution for deploying Inference Epoch 2** with one-click setup, **automatic restarts**, and **multi-instance execution**.  
It is a **fork of the [original Singosol project](https://github.com/singosol/kuzco-docker)** and has been optimized based on real-world testing to support the latest Epoch 2 updates.

### Key Improvements:
- Automatic detection of GPU VRAM  
- Multi-instance execution based on available memory  
- Fully commented for easy customization based on your hardware setup

---

## âœ… Requirements

- **Operating System**: Linux  
- **GPU Driver**: NVIDIA version 550  
- **GPU Memory**: More than 6GB (see [official hardware documentation](https://docs.inference.supply/hardware))  
- **Python**: Version 3.8 or higher  
- **Docker**: Latest version

---

## ðŸš€ Features

- ðŸ” **Auto-Restart** â€“ Runs continuously without manual intervention  
- ðŸ§© **Multi-Instance Support** â€“ Automatically allocates threads based on available VRAM  
  - Example: 6GB = 1 thread, 12GB = 2 threads, and so on

---

## ðŸ› ï¸ How to Use

Run the following command in your terminal:

```bash
python3 kzco.py -c "--worker XXX --code XXX"
```

> Replace `"--worker XXX --code XXX"` with the command generated by the **official Inference platform** when creating a new worker using the **Docker** option.

---
