# èƒ½ç»•è¿‡é™åˆ¶å¤šå¼€ï¼Œä½†éœ€è¦ä¸€äº›æŠ€å·§ï¼Œè¯·è€å¿ƒçœ‹å®Œã€‚
# Inferenceï¼ˆåŸ Kuzcoï¼‰Epoch 2 æœ€æ–°ä¸€é”®è‡ªåŠ¨é‡å¯ + å¤šå¼€è„šæœ¬

**ä½œè€…ï¼šJ1Nï¼ŒKuzCommunityCN Founder**  
**æ¨ç‰¹ï¼š[ @J1N226 ](https://twitter.com/J1N226)**ï¼ˆå…³æ³¨æ¨ç‰¹ç§ä¿¡æ‹‰äº¤æµç¾¤ï¼‰

---

## ğŸ“Œ é¡¹ç›®ç®€ä»‹

è¯¥è„šæœ¬ä¸º Inference Epoch 2 æä¾›äº† **ä¸€é”®éƒ¨ç½²ã€è‡ªåŠ¨é‡å¯å’Œå¤šå¼€** çš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚  
åŸºäº [Singosol åŸé¡¹ç›®](https://github.com/singosol/kuzco-docker) è¿›è¡Œ Forkï¼Œå¹¶æ ¹æ®å®æµ‹å¯¹å®˜æ–¹ Epoch 2 çš„æ›´æ–°è¿›è¡Œäº†é€‚é…ä¸ä¼˜åŒ–ã€‚

ä¸»è¦ä¼˜åŒ–å†…å®¹åŒ…æ‹¬ï¼š
- è‡ªåŠ¨é‡å¯
- æ ¹æ®æ˜¾å­˜è‡ªåŠ¨å¤šå¼€è¿è¡Œ
- æ”¯æŒå•å¡å¤šå¼€ã€å¤šå¡å¤šå¼€
- è„šæœ¬å†…å«è¯¦ç»†æ³¨é‡Šï¼Œæ–¹ä¾¿æ ¹æ®ä¸åŒè®¾å¤‡çµæ´»è°ƒæ•´å‚æ•°

---

## âœ… ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šLinux  
- **æ˜¾å¡é©±åŠ¨**ï¼šNVIDIA 550  
- **æ˜¾å­˜è¦æ±‚**ï¼šæ˜¾å­˜å¤§äº 6GBï¼ˆè¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.inference.supply/hardware)ï¼‰  
- **Python**ï¼š3.8 åŠä»¥ä¸Šç‰ˆæœ¬
- **Docker**ï¼šæœ€æ–°ç‰ˆ

---

## ğŸš€ è„šæœ¬åŠŸèƒ½

- ğŸ” **è‡ªåŠ¨é‡å¯**ï¼šæ— éœ€äººå·¥å¹²é¢„ï¼ŒæŒç»­ç¨³å®šè¿è¡Œ  
- ğŸ§© **å¤šå¼€æ”¯æŒ**ï¼šæ ¹æ®æ˜¾å­˜è‡ªåŠ¨åˆ†é…çº¿ç¨‹æ•°é‡  
  - ä¾‹å¦‚ï¼š6GB æ˜¾å­˜ = å•å¼€ï¼Œ12GB æ˜¾å­˜ = åŒå¼€ï¼Œä»¥æ­¤ç±»æ¨

---

## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

åœ¨ç»ˆç«¯è¾“å…¥ä»¥ä¸‹æŒ‡ä»¤è¿è¡Œè„šæœ¬ï¼š 

```bash
python3 kzco.py -c "--worker XXX --code XXX"
```

> å‚æ•° `"--worker XXX --code XXX"` æ¥è‡ªå®˜æ–¹ Inference å¹³å°åˆ›å»º Worker æ—¶é€‰æ‹© Docker æ–¹å¼æ‰€è‡ªåŠ¨ç”Ÿæˆçš„å‘½ä»¤ã€‚

---

## ğŸ“– æ³¨æ„äº‹é¡¹

- âœ… **æ”¯æŒå•å¡å¤šå¼€å’Œå¤šå¡å¤šå¼€**
- å½“ä½ **åˆ›å»ºä¸€ä¸ª Worker å¹¶è¿è¡Œä¸€æ¬¡è„šæœ¬æ—¶**ï¼Œä½ æœºå™¨ä¸Šçš„**æ¯å¼ æ˜¾å¡å°†å•å¼€**
- å¦‚æœä½ **å†åˆ›å»ºä¸€ä¸ªæ–°çš„ Worker å¹¶å†æ¬¡è¿è¡Œè„šæœ¬**ï¼Œæ¯å¼ æ˜¾å¡å°†ä¼š**åŒå¼€**
- é€šè¿‡é‡å¤åˆ›å»º Worker å’Œè¿è¡Œè„šæœ¬ï¼Œå¯ä»¥é€æ­¥å®ç°æ¯å¼ å¡çš„å¤šå¼€

### ğŸ“Š ä¸¾ä¾‹è¯´æ˜ï¼š

| æ˜¾å¡æ˜¾å­˜ | æ¯å¼ å¡æœ€å¤§è¿›ç¨‹æ•° | æ‰€éœ€ Worker æ•° |
|----------|------------------|----------------|
| 12GB     | 2                | 2              |
| 18GB     | 3                | 3              |
| 24GB     | 4                | 4              |

> ğŸ’¡ æ¯åˆ›å»ºä¸€ä¸ªæ–°çš„ Worker å¹¶å†æ¬¡è¿è¡Œè„šæœ¬ï¼Œç³»ç»Ÿå°±ä¼šä¸ºæ¯å¼ æ˜¾å¡ä¼šå¤šå¼€ä¸€ä¸ªè¿›ç¨‹


# Inference (formerly Kuzco) Epoch 2 One-Click Auto-Restart & Multi-Instance Script

**Author: J1N, Founder of KuzCommunityCN**  
**Twitter: [@J1N226](https://twitter.com/J1N226)** 

---

## ğŸ“Œ Overview

This script provides a **complete solution for deploying Inference Epoch 2** with one-click setup, **automatic restarts**, and **multi-instance execution**.  
It is a **fork of the [original Singosol project](https://github.com/singosol/kuzco-docker)** and has been optimized based on real-world testing to support the latest Epoch 2 updates.

### Key Improvements:
- Auto-Restart
- Multi-instance execution based on available memory  
- Fully commented for easy customization based on your hardware setup
- Supports multi-instance on a single GPU and across multiple GPUs

---

## âœ… Requirements

- **Operating System**: Linux  
- **GPU Driver**: NVIDIA version 550  
- **GPU Memory**: More than 6GB (see [official hardware documentation](https://docs.inference.supply/hardware))  
- **Python**: Version 3.8 or higher  
- **Docker**: Latest version

---

## ğŸš€ Features

- ğŸ” **Auto-Restart** â€“ Runs continuously without manual intervention  
- ğŸ§© **Multi-Instance Support** â€“ Automatically allocates threads based on available VRAM  
  - Example: 6GB = 1 thread, 12GB = 2 threads, and so on

---

## ğŸ› ï¸ How to Use

Run the following command in your terminal:

```bash
python3 kzco.py -c "--worker XXX --code XXX"
```

> Replace `"--worker XXX --code XXX"` with the command generated by the **official Inference platform** when creating a new worker using the **Docker** option.

---

## ğŸ“– Notes

- âœ… **Supports both single-GPU multi-instance and multi-GPU setups**
- When you **create one worker** and run the script once, **each GPU on your machine will launch one process**.
- When you **create a second worker** and run the script again, **each GPU will then run two processes**.
- Repeat this process to scale up the number of instances per GPU.

### ğŸ“Š Example:

| GPU VRAM | Max Instances | Required Workers |
|----------|---------------|------------------|
| 12GB     | 2             | 2                |
| 18GB     | 3             | 3                |
| 24GB     | 4             | 4                |

> ğŸ’¡ Each time you create a new worker and run the script again, the system adds another process per GPU.

